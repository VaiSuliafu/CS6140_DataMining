{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"D1.txt\",\"r\")\n",
    "s1 = file1.read()\n",
    "file1.close()\n",
    "\n",
    "file1 = open(\"D1.txt\",\"r\")\n",
    "s1 = file1.read()\n",
    "file1.close()\n",
    "\n",
    "file2 = open(\"D2.txt\",\"r\")\n",
    "s2 = file2.read()\n",
    "file2.close()\n",
    "\n",
    "file3 = open(\"D3.txt\",\"r\")\n",
    "s3 = file3.read()\n",
    "file3.close()\n",
    "\n",
    "file4 = open(\"D4.txt\",\"r\")\n",
    "s4 = file4.read()\n",
    "file4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCharGramList(s, size):\n",
    "    s_set = set()\n",
    "    for i in range(len(s)):\n",
    "        if(len(s[i:i+size]) == size):\n",
    "            s_set.add(s[i:i+size])\n",
    "        \n",
    "    return s_set\n",
    "\n",
    "def makeStringGram(s, size):\n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(size)])\n",
    "    \n",
    "    x = set()\n",
    "    for ngram in ngrams:\n",
    "        x.add(\" \".join(ngram))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size 2 char grams\n",
    "s1_set = makeCharGramList(s1, 2)\n",
    "s2_set = makeCharGramList(s2, 2)\n",
    "s3_set = makeCharGramList(s3, 2)\n",
    "s4_set = makeCharGramList(s4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_set3 = makeCharGramList(s1, 3)\n",
    "s2_set3 = makeCharGramList(s2, 3)\n",
    "s3_set3 = makeCharGramList(s3, 3)\n",
    "s4_set3 = makeCharGramList(s4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = makeStringGram(s1, 2)\n",
    "x2 = makeStringGram(s2, 2)\n",
    "x3 = makeStringGram(s3, 2)\n",
    "x4 = makeStringGram(s4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n",
      "264\n",
      "296\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "print(len(s1_set))\n",
    "print(len(s2_set))\n",
    "print(len(s3_set))\n",
    "print(len(s4_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773\n",
      "759\n",
      "978\n",
      "770\n"
     ]
    }
   ],
   "source": [
    "print(len(s1_set3))\n",
    "print(len(s2_set3))\n",
    "print(len(s3_set3))\n",
    "print(len(s4_set3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290\n",
      "297\n",
      "390\n",
      "364\n"
     ]
    }
   ],
   "source": [
    "print(len(x1))\n",
    "print(len(x2))\n",
    "print(len(x3))\n",
    "print(len(x4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count of unique set elements\n",
    "twoKCharSet = set()\n",
    "for element in s1_set:\n",
    "    twoKCharSet.add(element)\n",
    "for element in s2_set:\n",
    "    twoKCharSet.add(element)\n",
    "for element in s3_set:\n",
    "    twoKCharSet.add(element)\n",
    "for element in s4_set:\n",
    "    twoKCharSet.add(element)\n",
    "    \n",
    "# get the count of unique set elements\n",
    "threeKCharSet = set()\n",
    "for element in s1_set3:\n",
    "    threeKCharSet.add(element)\n",
    "for element in s2_set3:\n",
    "    threeKCharSet.add(element)\n",
    "for element in s3_set3:\n",
    "    threeKCharSet.add(element)\n",
    "for element in s4_set3:\n",
    "    threeKCharSet.add(element)\n",
    "    \n",
    "# get the count of unique set elements\n",
    "twoKWordSet = set()\n",
    "for element in x1:\n",
    "    twoKWordSet.add(element)\n",
    "for element in x2:\n",
    "    twoKWordSet.add(element)\n",
    "for element in x3:\n",
    "    twoKWordSet.add(element)\n",
    "for element in x4:\n",
    "    twoKWordSet.add(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n",
      "1470\n",
      "958\n"
     ]
    }
   ],
   "source": [
    "print(len(twoKCharSet))\n",
    "print(len(threeKCharSet))\n",
    "print(len(twoKWordSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = preprocessing.MultiLabelBinarizer()\n",
    "mlb3 = preprocessing.MultiLabelBinarizer()\n",
    "mlw = preprocessing.MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoCharArr = mlb.fit_transform([s1_set, s2_set, s3_set, s4_set])\n",
    "threeCharArr = mlb3.fit_transform([s1_set3, s2_set3, s3_set3, s4_set3])\n",
    "twoWordArr = mlw.fit_transform([x1, x2, x3, x4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'D1': twoCharArr[0], 'D2': twoCharArr[1], 'D3': twoCharArr[2], 'D4': twoCharArr[3]})\n",
    "df3 = pd.DataFrame({'D1': threeCharArr[0], 'D2': threeCharArr[1], 'D3': threeCharArr[2], 'D4': threeCharArr[3]})\n",
    "dfw = pd.DataFrame({'D1': twoWordArr[0], 'D2': twoWordArr[1], 'D3': twoWordArr[2], 'D4': twoWordArr[3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       ...,\n",
       "       [1, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twoCharArr = twoCharArr.transpose()\n",
    "threeCharArr = threeCharArr.transpose()\n",
    "twoWordArr = twoWordArr.transpose()\n",
    "twoCharArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twoCharArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_and(df, dnum, dnum2):\n",
    "    count = 0\n",
    "    for i in range(len(df[dnum])):\n",
    "        if (df[dnum][i] == 1 and df[dnum2][i] == 1):\n",
    "            count = count + 1.0\n",
    "    return count\n",
    "    \n",
    "def c_or(df, dnum, dnum2):\n",
    "    count = 0\n",
    "    for i in range(len(df[dnum])):\n",
    "        if (df[dnum][i] == 1 or df[dnum2][i] == 1):\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "def jaccardSimilarity(df, dnum, dnum2):\n",
    "    return c_and(df, dnum, dnum2) / c_or(df, dnum, dnum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9887640449438202\n",
      "0.7816455696202531\n",
      "0.6645161290322581\n",
      "0.7834394904458599\n",
      "0.6601941747572816\n",
      "0.6717791411042945\n"
     ]
    }
   ],
   "source": [
    "print(jaccardSimilarity(df, 'D1', 'D2'))\n",
    "print(jaccardSimilarity(df, 'D1', 'D3'))\n",
    "print(jaccardSimilarity(df, 'D1', 'D4'))\n",
    "print(jaccardSimilarity(df, 'D2', 'D3'))\n",
    "print(jaccardSimilarity(df, 'D2', 'D4'))\n",
    "print(jaccardSimilarity(df, 'D3', 'D4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9491094147582697\n",
      "0.5042955326460481\n",
      "0.3065198983911939\n",
      "0.4987057808455565\n",
      "0.3034953111679454\n",
      "0.31329827197595794\n"
     ]
    }
   ],
   "source": [
    "print(jaccardSimilarity(df3, 'D1', 'D2'))\n",
    "print(jaccardSimilarity(df3, 'D1', 'D3'))\n",
    "print(jaccardSimilarity(df3, 'D1', 'D4'))\n",
    "print(jaccardSimilarity(df3, 'D2', 'D3'))\n",
    "print(jaccardSimilarity(df3, 'D2', 'D4'))\n",
    "print(jaccardSimilarity(df3, 'D3', 'D4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78419452887538\n",
      "0.19507908611599298\n",
      "0.007704160246533128\n",
      "0.17636986301369864\n",
      "0.00916030534351145\n",
      "0.012080536912751677\n"
     ]
    }
   ],
   "source": [
    "print(jaccardSimilarity(dfw, 'D1', 'D2'))\n",
    "print(jaccardSimilarity(dfw, 'D1', 'D3'))\n",
    "print(jaccardSimilarity(dfw, 'D1', 'D4'))\n",
    "print(jaccardSimilarity(dfw, 'D2', 'D3'))\n",
    "print(jaccardSimilarity(dfw, 'D2', 'D4'))\n",
    "print(jaccardSimilarity(dfw, 'D3', 'D4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import random\n",
    "import math\n",
    "\n",
    "h20 = random.sample(range(1, 700), 20)\n",
    "h60 = random.sample(range(1, 700), 60)\n",
    "h150 = random.sample(range(1, 700), 150)\n",
    "h300 = random.sample(range(1, 700), 300)\n",
    "h600 = random.sample(range(1, 700), 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns an array of random hashes of r given the random sample.\n",
    "'''\n",
    "def h(r, samp):\n",
    "    h = [None] * len(samp)\n",
    "    for i in range(len(samp)):\n",
    "        samp_str = str(samp[i])\n",
    "        for j in range(len(samp_str)):\n",
    "            r = r+1 + (r+1 * int(samp_str[j]) * int(math.pow(3, j+1)))\n",
    "        h[i] = int(r) % 10001\n",
    "        \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Min hash algorithm simulation.\n",
    "'''\n",
    "def minHash(df, hashArr):\n",
    "    rows, cols = (len(hashArr), 4)\n",
    "    #M = [[99999]*cols]*rows # M(i, c) matrix\n",
    "    M = [[99999 for c in range(cols)] for rr in range(rows)] # M(i, c) matrix\n",
    "    for r in range(len(df)): # for each row 'r'\n",
    "        #print(\"ROW: \", r)\n",
    "        h_is = h(r, hashArr) # compute hash i's\n",
    "        for j in range(cols): # for each column 'j'\n",
    "            #print(\"COLUMNS: \", j)\n",
    "            if (df[r][j] == 1): # if df has a 1 at (r,j)\n",
    "                #print(\"DF[r][r]: \", df[r][j])\n",
    "                for i in range(len(h_is)): # for each hash_i\n",
    "                    #print(\"h\", i, \": \", h_is[i], \" vs M[\", i, \"][\", j, \"]:\", M[i][j], sep='')\n",
    "                    if (h_is[i] < M[i][j]):\n",
    "                        M[i][j] = h_is[i]\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = minHash(threeCharArr, h20)\n",
    "M2 = minHash(threeCharArr, h60)\n",
    "M3 = minHash(threeCharArr, h150)\n",
    "M4 = minHash(threeCharArr, h300)\n",
    "M5 = minHash(threeCharArr, h600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDF(df):\n",
    "    df = np.array(df)\n",
    "    df = pd.DataFrame(df)\n",
    "    df = df.rename(columns={0:\"D1\", 1:\"D2\", 2:\"D3\", 3:\"D4\"})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = makeDF(M1)\n",
    "M2 = makeDF(M2)\n",
    "M3 = makeDF(M3)\n",
    "M4 = makeDF(M4)\n",
    "M5 = makeDF(M5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000000\n",
      "1.0000000000000000\n",
      "0.8750000000000000\n",
      "0.9333333333333333\n",
      "0.9803921568627451\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:.16f}\".format(jaccardSimilarity(M1, 'D1', 'D2')))\n",
    "print(\"{0:.16f}\".format(jaccardSimilarity(M2, 'D1', 'D2')))\n",
    "print(\"{0:.16f}\".format(jaccardSimilarity(M3, 'D1', 'D2')))\n",
    "print(\"{0:.16f}\".format(jaccardSimilarity(M4, 'D1', 'D2')))\n",
    "print(\"{0:.16f}\".format(jaccardSimilarity(M5, 'D1', 'D2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 random numbers\n",
      "10000 loops, best of 5: 18.7 µs per loop\n",
      "\n",
      "60 random numbers\n",
      "10000 loops, best of 5: 47.4 µs per loop\n",
      "\n",
      "150 random numbers\n",
      "10000 loops, best of 5: 113 µs per loop\n",
      "\n",
      "300 random numbers\n",
      "1000 loops, best of 5: 210 µs per loop\n",
      "\n",
      "600 random numbers\n",
      "1000 loops, best of 5: 389 µs per loop\n",
      "\n",
      "900 random number\n",
      "1000 loops, best of 5: 578 µs per loop\n"
     ]
    }
   ],
   "source": [
    "print(\"20 random numbers\")\n",
    "%timeit random.sample(range(1, 700), 20)\n",
    "print(\"\")\n",
    "print(\"60 random numbers\")\n",
    "%timeit random.sample(range(1, 700), 60)\n",
    "print(\"\")\n",
    "print(\"150 random numbers\")\n",
    "%timeit random.sample(range(1, 700), 150)\n",
    "print(\"\")\n",
    "print(\"300 random numbers\")\n",
    "%timeit random.sample(range(1, 700), 300)\n",
    "print(\"\")\n",
    "print(\"600 random numbers\")\n",
    "%timeit random.sample(range(1, 700), 600)\n",
    "print(\"\")\n",
    "print(\"900 random number\")\n",
    "%timeit random.sample(range(1, 1000), 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9206349206349206\n"
     ]
    }
   ],
   "source": [
    "h900 = random.sample(range(1, 1000), 900)\n",
    "M6 = minHash(threeCharArr, h900)\n",
    "M6 = makeDF(M6)\n",
    "print(\"{0:.16f}\".format(jaccardSimilarity(M6, 'D1', 'D2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
